# TEMPORAL: Time-Embedded Tokens for Experiential Learning

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A novel neural architecture that uses **time-embedded tokens** to enable **experiential learning during inference**. Tokens accumulate experience through usage, allowing models to "know what they know."

## ğŸ¯ Core Idea

Replace standard token embeddings with dual-component representations:

```
Standard Embedding:  token â†’ [content(256)]
TEMPORAL Embedding:  token â†’ [content(128) | time(128)]
```

- **Content embeddings**: What the token IS (learned via backprop)
- **Time embeddings**: How EXPERIENCED it is (updated via usage)

Time embeddings start at zero and grow with each token use, creating a form of **stateful, experiential learning** without requiring gradient computation.

## ğŸš€ Quick Start

```bash
cd temporal_prototype

# Install dependencies
pip install -r requirements.txt

# Run tests
python test_implementation.py

# Train both TEMPORAL and Baseline models
python train.py --model both

# Evaluate and compare
python evaluate.py --model both

# Generate visualizations
python visualize.py
```

See [QUICKSTART.md](temporal_prototype/QUICKSTART.md) for detailed instructions.

## ğŸ“Š What Makes TEMPORAL Novel?

### 1. Dual-Component Tokens

Every token has two parts:
- **Content (128d)**: Traditional semantic embedding
- **Time (128d)**: Experience accumulation vector

### 2. Usage-Based Time Updates

Time embeddings update during forward pass:

```python
time_emb[token] += lr * [usage_count, recency, diversity, confidence, ...]
```

No backprop required - lightweight and efficient!

### 3. Experiential Learning

Frequent tokens develop:
- âœ… Higher time values
- âœ… Better prediction accuracy
- âœ… Higher model confidence

The model learns through **experience**, not just gradients.

### 4. Epistemic Awareness

TEMPORAL models can track their own knowledge:
- High time value = "I know this token well"
- Low time value = "This is unfamiliar to me"

Foundation for uncertainty quantification and calibrated confidence.

## ğŸ“ˆ Key Results

Expected outcomes from the prototype (10M parameters, WikiText-2):

| Metric | TEMPORAL | Baseline | Result |
|--------|----------|----------|--------|
| Test Perplexity | ~69 | ~72 | âœ… 4-5% better |
| Time-Frequency Corr. | 0.85+ | N/A | âœ… Strong correlation |
| High-Time Token Acc. | 87%+ | N/A | âœ… Better on familiar tokens |
| Confidence-Time Corr. | 0.65+ | N/A | âœ… Knows what it knows |

*Note: Exact values depend on training run and dataset*

## ğŸ—ï¸ Architecture

```
Input Tokens
    â†“
[Content Emb. | Time Emb.]  â† Dual 256d representation
    â†“
Positional Encoding
    â†“
Time-Aware Attention (Ã—2 layers)
    â†“
Layer Norm + FFN
    â†“
Output Logits
    â†“
Update Time Embeddings  â† Key innovation!
```

### Model Specifications

- **Parameters**: ~10M (prototype scale)
- **Layers**: 2 transformer blocks
- **Heads**: 4 attention heads
- **Dimensions**: 128 content + 128 time = 256 total
- **Vocabulary**: 1,000 tokens (configurable)
- **Context**: 128 tokens

## ğŸ“ Project Structure

```
temporal_prototype/
â”œâ”€â”€ config.py              # Hyperparameters
â”œâ”€â”€ time_embeddings.py     # Time embedding layer
â”œâ”€â”€ model.py              # TEMPORAL & Baseline transformers
â”œâ”€â”€ train.py              # Training script
â”œâ”€â”€ evaluate.py           # Evaluation with metrics
â”œâ”€â”€ visualize.py          # Plotting and analysis
â”œâ”€â”€ test_implementation.py # Unit tests
â”œâ”€â”€ check_syntax.py       # Syntax validation
â”œâ”€â”€ requirements.txt      # Dependencies
â”œâ”€â”€ README.md            # Main documentation
â”œâ”€â”€ QUICKSTART.md        # Getting started guide
â””â”€â”€ ANALYSIS.md          # Results template
```

## ğŸ”¬ Experiment Workflow

1. **Train**: `python train.py --model both`
   - Trains TEMPORAL and Baseline
   - Logs time evolution, perplexity, loss
   - Saves checkpoints every N steps

2. **Evaluate**: `python evaluate.py --model both`
   - Computes test perplexity
   - Analyzes time-frequency correlation
   - Measures confidence-time relationship
   - Categorizes tokens by experience level

3. **Visualize**: `python visualize.py`
   - Time embedding evolution plots
   - Frequency vs time scatter plots
   - Perplexity comparison curves
   - Token category analysis bars
   - Comprehensive summary figure

4. **Analyze**: Review `outputs/evaluation_results.json` and plots

## ğŸ“Š Tracked Metrics

### Standard Metrics
- Training/validation loss
- Test perplexity
- Parameter count

### TEMPORAL-Specific Metrics

1. **Time Evolution**: How time embeddings grow during training
2. **Frequency-Time Correlation**: Do frequent tokens have high time?
3. **Accuracy by Time Category**: Performance on experienced vs new tokens
4. **Confidence-Time Correlation**: Does confidence track experience?

## ğŸ¨ Visualizations

The prototype generates:

- `time_evolution.png` - Time magnitude growth over epochs
- `frequency_vs_time.png` - Token usage vs time value correlation
- `time_distribution.png` - Histogram of time values
- `perplexity_comparison.png` - TEMPORAL vs Baseline learning curves
- `token_category_analysis.png` - Accuracy/confidence by time category
- `summary_analysis.png` - Comprehensive 6-panel summary

## ğŸ”§ Configuration

Edit `config.py` to customize:

```python
# Model architecture
content_dim = 128      # Content embedding size
time_dim = 128        # Time embedding size
n_layers = 2          # Transformer layers
n_heads = 4           # Attention heads

# Training
num_epochs = 10
batch_size = 32
learning_rate = 0.0003  # For content embeddings
time_lr = 0.01         # For time embeddings

# Time update mechanism
# Dimensions 0-3: Manual (usage, recency, diversity, confidence)
# Dimensions 4-127: Learned via gradients
```

## ğŸ§ª Testing

```bash
# Syntax check (no dependencies required)
python check_syntax.py

# Full unit tests (requires PyTorch)
python test_implementation.py
```

Test coverage:
- âœ… Time embedding initialization and updates
- âœ… Dual token representation [content | time]
- âœ… TEMPORAL transformer forward pass
- âœ… Baseline transformer comparison
- âœ… Time value increase with usage
- âœ… Gradient flow through architecture

## ğŸš€ Future Directions

### Immediate Extensions
- **Time Decay**: Forgetting mechanism for unused tokens
- **Multi-Modal Time**: Separate time for different contexts
- **Adaptive LR**: Vary time updates based on confidence
- **Inference Learning**: Continue time updates during deployment

### Scaling
- Larger models (100M+ parameters)
- Full vocabularies (50k+ tokens)
- Longer contexts (2k+ tokens)
- Multiple datasets and domains

### Research Questions
- How does time interact with positional encodings?
- Can time embeddings transfer across tasks?
- What is optimal time dimensionality?
- How does multi-layer time differ?

## ğŸ“š Key Files

- **[README.md](temporal_prototype/README.md)**: Detailed documentation
- **[QUICKSTART.md](temporal_prototype/QUICKSTART.md)**: Step-by-step guide
- **[ANALYSIS.md](temporal_prototype/ANALYSIS.md)**: Results template and interpretation
- **[requirements.txt](temporal_prototype/requirements.txt)**: Python dependencies

## ğŸ¤ Contributing

This is a research prototype. Contributions, experiments, and extensions welcome!

Areas for contribution:
- Time update mechanisms
- Novel attention patterns
- Scaling experiments
- Alternative applications
- Theoretical analysis

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

Inspired by:
- Transformer architecture (Vaswani et al., 2017)
- Episodic memory in neural networks
- Meta-learning and few-shot learning
- Uncertainty quantification in deep learning

## ğŸ“§ Contact

For questions, discussions, or collaborations, please open an issue.

---

## ğŸ¯ Success Criteria Summary

TEMPORAL is successful if:

- [x] **Implementation Complete**: All components working
- [ ] **Time Tracks Usage**: Frequent tokens â†’ high time values
- [ ] **Competitive Performance**: Perplexity â‰¤ baseline
- [ ] **Epistemic Awareness**: Confidence correlates with experience
- [ ] **Experiential Learning**: Better performance on familiar tokens

Run the prototype to validate these criteria! ğŸš€

---

**Built with**: PyTorch | NumPy | Matplotlib | â¤ï¸
